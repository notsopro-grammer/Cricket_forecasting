{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f500617",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_country_dict = {\n",
    "    'Harare Sports Club': 'Zimbabwe',\n",
    "    'Green Park': 'India',\n",
    "    'Vidarbha Cricket Association Stadium, Jamtha': 'India',\n",
    "    'M Chinnaswamy Stadium': 'India',\n",
    "    'Central Broward Regional Park Stadium Turf Ground': 'USA',\n",
    "    'Sabina Park, Kingston': 'Jamaica',\n",
    "    'R.Premadasa Stadium, Khettarama': 'Sri Lanka',\n",
    "    'JSCA International Stadium Complex': 'India',\n",
    "    'Barsapara Cricket Stadium': 'India',\n",
    "    'Old Trafford': 'England',\n",
    "    'Sophia Gardens': 'England',\n",
    "    'County Ground': 'England',\n",
    "    'Arun Jaitley Stadium': 'India',\n",
    "    'Saurashtra Cricket Association Stadium': 'India',\n",
    "    'Greenfield International Stadium': 'India',\n",
    "    'The Wanderers Stadium': 'South Africa',\n",
    "    'SuperSport Park': 'South Africa',\n",
    "    'Newlands': 'South Africa',\n",
    "    'Barabati Stadium': 'India',\n",
    "    'Holkar Cricket Stadium': 'India',\n",
    "    'Wankhede Stadium': 'India',\n",
    "    'The Village, Malahide': 'Ireland',\n",
    "    'Brisbane Cricket Ground, Woolloongabba': 'Australia',\n",
    "    'Melbourne Cricket Ground': 'Australia',\n",
    "    'Sydney Cricket Ground': 'Australia',\n",
    "    'Westpac Stadium': 'New Zealand',\n",
    "    'Eden Park': 'New Zealand',\n",
    "    'Seddon Park': 'New Zealand',\n",
    "    'Eden Gardens': 'India',\n",
    "    'Bharat Ratna Shri Atal Bihari Vajpayee Ekana Cricket Stadium': 'India',\n",
    "    'MA Chidambaram Stadium, Chepauk': 'India',\n",
    "    'Dr. Y.S. Rajasekhara Reddy ACA-VDCA Cricket Stadium': 'India',\n",
    "    'M.Chinnaswamy Stadium': 'India',\n",
    "    'Punjab Cricket Association IS Bindra Stadium, Mohali': 'India',\n",
    "    'Rajiv Gandhi International Stadium, Uppal': 'India',\n",
    "    'Bay Oval': 'New Zealand',\n",
    "    'Providence Stadium, Guyana': 'Guyana',\n",
    "    'Maharashtra Cricket Association Stadium': 'India',\n",
    "    'Manuka Oval': 'Australia',\n",
    "    'Narendra Modi Stadium': 'India',\n",
    "    'R Premadasa Stadium, Colombo': 'Sri Lanka',\n",
    "    'Dubai International Cricket Stadium': 'UAE',\n",
    "    'The Rose Bowl, Southampton': 'England',\n",
    "    'Edgbaston, Birmingham': 'England',\n",
    "    'Trent Bridge, Nottingham': 'England',\n",
    "    'Sawai Mansingh Stadium, Jaipur': 'India',\n",
    "    'JSCA International Stadium Complex, Ranchi': 'India',\n",
    "    'Eden Gardens, Kolkata': 'India',\n",
    "    'Bharat Ratna Shri Atal Bihari Vajpayee Ekana Cricket Stadium, Lucknow': 'India',\n",
    "    'Himachal Pradesh Cricket Association Stadium, Dharamsala': 'India',\n",
    "    'Arun Jaitley Stadium, Delhi': 'India',\n",
    "    'Barabati Stadium, Cuttack': 'India',\n",
    "    'Dr. Y.S. Rajasekhara Reddy ACA-VDCA Cricket Stadium, Visakhapatnam': 'India',\n",
    "    'Saurashtra Cricket Association Stadium, Rajkot': 'India',\n",
    "    'M Chinnaswamy Stadium, Bangalore': 'India',\n",
    "    'Perth Stadium': 'Australia',\n",
    "    'Adelaide Oval': 'Australia',\n",
    "    'The Village, Malahide, Dublin': 'Ireland',\n",
    "    'Brian Lara Stadium, Tarouba, Trinidad': 'Trinidad and Tobago',\n",
    "    'Warner Park, Basseterre, St Kitts': 'Saint Kitts and Nevis',\n",
    "    'Central Broward Regional Park Stadium Turf Ground, Lauderhill': 'USA',\n",
    "    'Bay Oval, Mount Maunganui': 'New Zealand',\n",
    "    'McLean Park, Napier': 'New Zealand',\n",
    "    'Punjab Cricket Association IS Bindra Stadium, Mohali, Chandigarh': 'India',\n",
    "    'Vidarbha Cricket Association Stadium, Jamtha, Nagpur': 'India',\n",
    "    'Rajiv Gandhi International Stadium, Uppal, Hyderabad': 'India',\n",
    "    'Greenfield International Stadium, Thiruvananthapuram': 'India',\n",
    "    'Barsapara Cricket Stadium, Guwahati': 'India',\n",
    "    'Holkar Cricket Stadium, Indore': 'India',\n",
    "    'Wankhede Stadium, Mumbai': 'India',\n",
    "    'Maharashtra Cricket Association Stadium, Pune': 'India',\n",
    "    'Narendra Modi Stadium, Ahmedabad': 'India',\n",
    "    'Malahide, Dublin': 'Ireland',\n",
    "    \"St George's Park, Gqeberha\": 'South Africa',\n",
    "    'The Wanderers Stadium, Johannesburg': 'South Africa',\n",
    "    'Shaheed Veer Narayan Singh International Stadium, Raipur': 'India',\n",
    "    'M Chinnaswamy Stadium, Bengaluru': 'India',\n",
    "    'Zhejiang University of Technology Cricket Field': 'China',\n",
    "    'Nassau County International Cricket Stadium, New York': 'USA',\n",
    "    'Sir Vivian Richards Stadium, North Sound, Antigua': 'Antigua and Barbuda',\n",
    "    'Daren Sammy National Cricket Stadium, Gros Islet, St Lucia': 'Saint Lucia',\n",
    "    'Kensington Oval, Bridgetown, Barbados': 'Barbados',\n",
    "    'Shrimant Madhavrao Scindia Cricket Stadium, Gwalior': 'India',\n",
    "    'Pallekele International Cricket Stadium': 'Sri Lanka',\n",
    "    'Kingsmead, Durban': 'South Africa',\n",
    "    'SuperSport Park, Centurion': 'South Africa',\n",
    "    'New Wanderers Stadium': 'South Africa',\n",
    "    'Kingsmead': 'South Africa',\n",
    "    'Brabourne Stadium': 'India',\n",
    "    'Trent Bridge': 'England',\n",
    "    \"Lord's\": 'England',\n",
    "    'AMI Stadium': 'New Zealand',\n",
    "    'R Premadasa Stadium': 'Sri Lanka',\n",
    "    'Beausejour Stadium, Gros Islet': 'Saint Lucia',\n",
    "    'Kensington Oval, Bridgetown': 'Barbados',\n",
    "    'Punjab Cricket Association Stadium, Mohali': 'India',\n",
    "    'Moses Mabhida Stadium': 'South Africa',\n",
    "    \"Queen's Park Oval, Port of Spain\": 'Trinidad and Tobago',\n",
    "    'Stadium Australia': 'Australia',\n",
    "    'Subrata Roy Sahara Stadium': 'India',\n",
    "    'Sardar Patel Stadium, Motera': 'India',\n",
    "    'Edgbaston': 'England',\n",
    "    'Shere Bangla National Stadium': 'Bangladesh',\n",
    "    'Himachal Pradesh Cricket Association Stadium': 'India'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedFeatureExtractor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def extract_advanced_features(self):\n",
    "        # 1. Momentum: average runs over the last 5 balls\n",
    "        self.df['runs_last_5_balls'] = (\n",
    "            self.df.groupby(['match_id', 'innings'])['total_run'].rolling(window=5, min_periods=1).mean().reset_index(0, drop=True).values\n",
    "        )\n",
    "\n",
    "        # 2. Momentum: total wickets taken over the last 20 balls\n",
    "        self.df['wickets_last_20_balls'] = (\n",
    "            self.df.groupby(['match_id', 'innings'])['is_wicket'].rolling(window=20, min_periods=1).sum().reset_index(0, drop=True).values\n",
    "        )\n",
    "\n",
    "        # 3. Venue scoring tendency: how the current venueâ€™s average run rate compares to overall\n",
    "        venue_avg_score = self.df.groupby('venue')['total_run'].transform('mean')\n",
    "        self.df['venue_score_impact'] = venue_avg_score / venue_avg_score.mean()\n",
    "\n",
    "        # 4. Partnership runs: cumulative runs by the current striker in this innings\n",
    "        self.df['partnership_runs'] = (\n",
    "            self.df.groupby(['match_id', 'innings', 'striker_id'])['total_run'].cumsum()\n",
    "        )\n",
    "\n",
    "        # 5. Running run rate: total runs so far divided by overs bowled\n",
    "        self.df['current_run_rate'] = (\n",
    "            self.df.groupby(['match_id', 'innings'])['total_run']\n",
    "            .cumsum()/ (self.df['total_balls'] / 6)\n",
    "            )\n",
    "\n",
    "        # 6. Wickets remaining: 10 total wickets minus those fallen so far\n",
    "        self.df['wickets_remaining'] = (10 - self.df.groupby(['match_id', 'innings'])['is_wicket'].cumsum())\n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d492a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_season(season_str):\n",
    "    if not isinstance(season_str, str):\n",
    "        return None\n",
    "    parts = season_str.split('/')\n",
    "    if len(parts) == 2:\n",
    "        return 2000 + int(parts[1])\n",
    "    try:\n",
    "        return int(parts[0])\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26792c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wicket_flag(df: pd.DataFrame):\n",
    "    df['is_wicket'] = 0\n",
    "    if 'player_dismissed' in df.columns:\n",
    "        df.loc[df['player_dismissed'].notna(), 'is_wicket'] = 1\n",
    "    if 'other_player_dismissed' in df.columns:\n",
    "        df.loc[df['other_player_dismissed'].notna(), 'is_wicket'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_add_recency(csv_path: str):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['season'] = df['season'].apply(parse_season)\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], format='%d-%m-%Y', errors='coerce')\n",
    "    df.sort_values(by=['start_date', 'match_id', 'innings', 'total_balls'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    earliest_date = df['start_date'].min()\n",
    "    df['days_since'] = (df['start_date'] - earliest_date).dt.days\n",
    "    match_min_days = df.groupby('match_id')['days_since'].transform('min')\n",
    "    max_days = match_min_days.max() if not match_min_days.empty else 1\n",
    "    df['recency_norm'] = match_min_days / max_days\n",
    "    alpha = 1.0\n",
    "    df['recency_weight'] = 1.0 + alpha * df['recency_norm']\n",
    "\n",
    "    df = create_wicket_flag(df)\n",
    "    feature_extractor = EnhancedFeatureExtractor(df)\n",
    "    df = feature_extractor.extract_advanced_features()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a49bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_columns(df: pd.DataFrame, cat_cols: list):\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna(\"NaNPlaceholder\")\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            encoders[col] = le\n",
    "    return df, encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_columns(df: pd.DataFrame):\n",
    "    numeric_cols = [\n",
    "        'season', 'innings', 'total_balls', 'runs_off_bat', 'extras',\n",
    "        'wickets_fallen', 'current_score', 'runs_conceded', 'strike_rate',\n",
    "        'run_rate', 'economy', 'home/away', 'current_run_rate', 'wickets_remaining',\n",
    "        'bowler_id', 'striker_id', 'non_striker_id'\n",
    "    ]\n",
    "    cat_cols = [\n",
    "        'batting_team', 'bowling_team', 'venue', 'toss_winner',\n",
    "        'toss_decision', 'day/night'\n",
    "    ]\n",
    "    numeric_cols = [c for c in numeric_cols if c in df.columns]\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "    return numeric_cols, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36472365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame):\n",
    "    # 1. Flag wickets\n",
    "    df = create_wicket_flag(df)\n",
    "\n",
    "    # 2. Derive city from venue, then collapse smaller West Indies countries\n",
    "    df['city'] = df['venue'].replace(venue_country_dict)\n",
    "    west_indies = [\n",
    "        'Guyana', 'Trinidad and Tobago', 'Barbados',\n",
    "        'Saint Lucia', 'Saint Kitts and Nevis',\n",
    "        'Antigua and Barbuda', 'Jamaica'\n",
    "    ]\n",
    "    df.loc[df['city'].isin(west_indies), 'city'] = 'West Indies'\n",
    "\n",
    "    # 3. Home/Away indicator: India home = 1, else away = 0\n",
    "    df['home/away'] = np.where(df['city'] == 'India', 1, 0)\n",
    "\n",
    "    # 4. Preserve original innings label for later filtering\n",
    "    df['original_innings'] = df['innings']\n",
    "\n",
    "    # 5. Determine which columns to treat as numeric vs. categorical\n",
    "    numeric_cols, cat_cols = build_feature_columns(df)\n",
    "\n",
    "    # 6. Encode all categorical columns into integer codes\n",
    "    df, encoders = encode_categorical_columns(df, cat_cols)\n",
    "\n",
    "    # 7. Clean numeric columns:\n",
    "    #    - Replace NaNs with 0\n",
    "    #    - Replace infinities with 0\n",
    "    #    - Clip extreme values to [-500, 500]\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "    df[numeric_cols] = df[numeric_cols].clip(lower=-500, upper=500)\n",
    "\n",
    "    # 8. Standardize numeric columns (zero mean, unit variance)\n",
    "    scaler = StandardScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # 9. Final list of features to use in modeling\n",
    "    feature_cols = numeric_cols + cat_cols\n",
    "    return df, feature_cols, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (max_len, d_model) with positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the div_term for even indices\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        # Apply sine to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add a batch dimension (1, max_len, d_model) and register as buffer\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        # Slice the positional encodings to the current sequence length\n",
    "        pe = self.pe[:, :seq_len, :].to(x.device)\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69967def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCricketLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.7, beta=0.1, gamma=0.5, delta=0.6):\n",
    "        super().__init__()\n",
    "        # Huber loss (Smooth L1) without reduction, so we can weight per sample\n",
    "        self.huber_loss = nn.SmoothL1Loss(reduction='none')\n",
    "        self.alpha = alpha    # weight for MAPE term\n",
    "        self.beta = beta      # weight for Huber term\n",
    "        self.gamma = gamma    # penalty multiplier for under-prediction\n",
    "        self.delta = delta    # penalty multiplier for over-prediction\n",
    "\n",
    "    def forward(self, pred, target, weight=None, run_rate=None, wickets_remaining=None):\n",
    "        # 1. Huber loss per sample\n",
    "        huber = self.huber_loss(pred, target)  \n",
    "        \n",
    "        # 2. MAPE: absolute percentage error Ã—100\n",
    "        mape = torch.abs((pred - target) / (target + 1e-8)) * 100\n",
    "        \n",
    "        # 3. Directional penalties:\n",
    "        #    under-prediction penalty: (target - pred) when pred <= target\n",
    "        underprediction_penalty = (target - pred).clamp(min=0) * self.gamma\n",
    "        #    over-prediction penalty: (pred - target) when pred >= target\n",
    "        overprediction_penalty = (pred - target).clamp(min=0) * self.delta\n",
    "\n",
    "        # 4. Contextual scaling (if provided):\n",
    "        if run_rate is not None and wickets_remaining is not None:\n",
    "            # High run rate â†’ under-prediction is costlier\n",
    "            underprediction_penalty *= (1 + run_rate / 6.0)\n",
    "            # Few wickets remaining â†’ over-prediction is costlier\n",
    "            overprediction_penalty *= (1 + (10 - wickets_remaining) / 10.0)\n",
    "\n",
    "        # 5. Combine terms, applying recency weight if given\n",
    "        if weight is not None:\n",
    "            combined = (\n",
    "                self.alpha * (mape * weight) +\n",
    "                self.beta * (huber * weight) +\n",
    "                underprediction_penalty * weight +\n",
    "                overprediction_penalty * weight\n",
    "            )\n",
    "        else:\n",
    "            combined = (\n",
    "                self.alpha * mape +\n",
    "                self.beta * huber +\n",
    "                underprediction_penalty +\n",
    "                overprediction_penalty\n",
    "            )\n",
    "\n",
    "        # 6. Return mean loss over the batch\n",
    "        return combined.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerHybridModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_lstm_layers=2,\n",
    "                 num_transformer_layers=1, nhead=8, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Input projection: map raw features to a unified hidden dimension\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # 2) Bidirectional LSTM stack for local sequence modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_lstm_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # 3) Learnable [CLS] token to summarize the sequence\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size * 2))\n",
    "\n",
    "        # 4) Sinusoidal positional encoding for Transformer\n",
    "        self.positional_encoding = PositionalEncoding(hidden_size * 2)\n",
    "\n",
    "        # 5) Transformer encoder for longâ€‘range dependencies\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size * 2,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "\n",
    "        # 6) Regression head: convert the [CLS] output into a scalar prediction\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len=30, input_size)\n",
    "        x = self.input_projection(x)\n",
    "        # After projection: (batch_size, 30, hidden_size)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # After BiLSTM: (batch_size, 30, hidden_size*2)\n",
    "\n",
    "        # Prepend a batchâ€‘wise expanded [CLS] token\n",
    "        batch_size = x.size(0)\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Shape: (batch_size, 1, hidden_size*2)\n",
    "        x = torch.cat([cls_token, lstm_out], dim=1)\n",
    "        # Now x.shape = (batch_size, 31, hidden_size*2)\n",
    "\n",
    "        # Inject positional information\n",
    "        x = self.positional_encoding(x)\n",
    "        # Shape remains (batch_size, 31, hidden_size*2)\n",
    "\n",
    "        # Transformer encoder applies selfâ€‘attention over all 31 positions\n",
    "        encoder_out = self.transformer_encoder(x)\n",
    "        # Shape: (batch_size, 31, hidden_size*2)\n",
    "\n",
    "        # Take the [CLS] output (position 0) as the sequence summary\n",
    "        cls_output = encoder_out[:, 0, :]  \n",
    "        # Shape: (batch_size, hidden_size*2)\n",
    "\n",
    "        # Feed through regression head to predict the final score\n",
    "        output = self.output_net(cls_output).squeeze(-1)\n",
    "        # Shape: (batch_size,)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CricketBallDatasetWithWeight(Dataset):\n",
    "    def __init__(self, X, y_final_score, weights, run_rates, wickets_remaining):\n",
    "        self.X = X\n",
    "        self.y_final_score = y_final_score\n",
    "        self.weights = weights\n",
    "        self.run_rates = run_rates\n",
    "        self.wickets_remaining = wickets_remaining\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "                (torch.tensor(self.y_final_score[idx], dtype=torch.float32),\n",
    "                 torch.tensor(self.weights[idx], dtype=torch.float32),\n",
    "                 torch.tensor(self.run_rates[idx], dtype=torch.float32),\n",
    "                 torch.tensor(self.wickets_remaining[idx], dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_with_weight(df: pd.DataFrame,feature_cols: list,seq_length=30,runs_col='total_run',weight_col='recency_weight'):\n",
    "    X_list, y_final_score_list, weight_list, run_rate_list, wickets_remaining_list, meta_list = [], [], [], [], [], []\n",
    "    grouped = df.groupby(['match_id', 'innings'], sort=False)\n",
    "\n",
    "    for (m_id, inn), group_data in grouped:\n",
    "        # 1. Sort deliveries within this innings by the ball index\n",
    "        group_data = group_data.sort_values('total_balls')\n",
    "\n",
    "        # 2. Extract raw arrays for features, runs, and context\n",
    "        feats = group_data[feature_cols].values                # shape: (num_balls, num_features)\n",
    "        runs = group_data[runs_col].values                     # run scored on each ball\n",
    "        group_weight = group_data[weight_col].iloc[0]          # same recency weight for all balls in this match\n",
    "        final_score = np.sum(runs)                             # target: total runs in the innings\n",
    "        orig_inn = group_data['original_innings'].iloc[0]      # store 1st/2nd innings label\n",
    "        run_rates = group_data['current_run_rate'].values      # dynamic run rate at each ball\n",
    "        wickets_remaining = group_data['wickets_remaining'].values  # wickets still in hand at each ball\n",
    "\n",
    "        # 3. Slide a window across this innings to create sequences\n",
    "        #    Step size = 6 balls (one over), window length = seq_length (30 balls)\n",
    "        for start_idx in range(0, len(group_data) - seq_length, 6):\n",
    "            end_idx = start_idx + seq_length\n",
    "\n",
    "            # 3a. Input sequence: features for balls [start_idx, end_idx)\n",
    "            X_seq = feats[start_idx:end_idx]\n",
    "            X_list.append(X_seq)\n",
    "\n",
    "            # 3b. Target: the final innings score for every sequence\n",
    "            y_final_score_list.append(final_score)\n",
    "\n",
    "            # 3c. Recency weight for this sequence\n",
    "            weight_list.append(group_weight)\n",
    "\n",
    "            # 3d. Context at window end: run rate and wickets remaining\n",
    "            run_rate_list.append(run_rates[end_idx - 1])\n",
    "            wickets_remaining_list.append(wickets_remaining[end_idx - 1])\n",
    "\n",
    "            # 3e. Metadata: match ID, original innings, starting ball index\n",
    "            meta_list.append((m_id, orig_inn, start_idx))\n",
    "\n",
    "    # 4. Convert lists into NumPy arrays for model input\n",
    "    X_array = np.array(X_list, dtype=np.float32)                      # shape: (num_sequences, 30, num_features)\n",
    "    y_final_score_array = np.array(y_final_score_list, dtype=np.float32)\n",
    "    weight_array = np.array(weight_list, dtype=np.float32)\n",
    "    run_rate_array = np.array(run_rate_list, dtype=np.float32)\n",
    "    wickets_remaining_array = np.array(wickets_remaining_list, dtype=np.float32)\n",
    "\n",
    "    return (X_array,y_final_score_array,weight_array,run_rate_array,wickets_remaining_array,meta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_model(model,train_loader,val_loader=None,epochs=50,lr=1e-3,device='cpu',max_grad_norm=1.0,patience=10):\n",
    "    # 1. Move the model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "\n",
    "    # 2. Set up the optimizer: AdamW with weight decay for regularization\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                            lr=lr,\n",
    "                            weight_decay=0.1)\n",
    "\n",
    "    # 3. Learningâ€‘rate scheduler: Cosine Annealing with warm restarts\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=5,       # after 5 epochs, restart the LR schedule\n",
    "        T_mult=2,    # each subsequent restart cycle is twice as long\n",
    "        eta_min=1e-6 # minimum LR after annealing\n",
    "    )\n",
    "\n",
    "    # 4. Use your custom cricket loss\n",
    "    criterion = CustomCricketLoss(alpha=0.7,\n",
    "                                 beta=0.1,\n",
    "                                 gamma=0.5,\n",
    "                                 delta=0.6)\n",
    "\n",
    "    # 5. Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # 6. Epoch loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()               # set model to training mode\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        # 7. Batch loop over training data\n",
    "        for X_batch, (y_batch, w_batch, rr_batch, wr_batch) in train_loader:\n",
    "            # 7a. Move batch tensors to device\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            w_batch = w_batch.to(device)\n",
    "            rr_batch = rr_batch.to(device)\n",
    "            wr_batch = wr_batch.to(device)\n",
    "\n",
    "            # 7b. Zero out any gradients from previous step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 7c. Forward pass: get predictions\n",
    "            pred = model(X_batch)\n",
    "\n",
    "            # 7d. Compute loss (includes recency weight, run rate, wickets)\n",
    "            loss = criterion(pred, y_batch, w_batch, rr_batch, wr_batch)\n",
    "\n",
    "            # 7e. Safety check: abort if loss is NaN or infinite\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"NaN/Inf loss detected at epoch {epoch+1}\")\n",
    "                return None, None, None\n",
    "\n",
    "            # 7f. Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 7g. Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            # 7h. Optimizer step: update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # 7i. Track this batchâ€™s loss\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # 8. Scheduler step: adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # 9. Compute and log average training loss for this epoch\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # 10. If a validation loader is provided, evaluate on validation set\n",
    "        if val_loader:\n",
    "            model.eval()           # switch to evaluation mode\n",
    "            epoch_val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_val, (y_val, w_val, rr_val, wr_val) in val_loader:\n",
    "                    # Move to device\n",
    "                    X_val = X_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    w_val = w_val.to(device)\n",
    "                    rr_val = rr_val.to(device)\n",
    "                    wr_val = wr_val.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    pred = model(X_val)\n",
    "                    # Compute validation loss\n",
    "                    val_loss = criterion(pred, y_val, w_val, rr_val, wr_val)\n",
    "                    epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "            # Compute and log average validation loss\n",
    "            avg_val_loss = np.mean(epoch_val_losses)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # 11. Earlyâ€‘stopping logic\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0     # reset patience if improved\n",
    "            else:\n",
    "                patience_counter += 1    # otherwise, increment\n",
    "\n",
    "        # 12. If no improvement for `patience` epochs, stop training\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # 13. Return the trained model and loss histories\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d628ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_espn_data(espn_file_path, match_id, innings):\n",
    "    espn_df = pd.read_excel(espn_file_path)\n",
    "    filtered_data = espn_df[(espn_df['match_id'] == match_id) & (espn_df['innings'] == innings)]\n",
    "    return filtered_data[['over', 'espn_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ae5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_over_wise_predictions(ball_indices, predictions, seq_length=30):\n",
    "    over_predictions = {}\n",
    "    for ball_idx, prediction in zip(ball_indices, predictions):\n",
    "        over = (ball_idx // 6) + 1\n",
    "        ball_in_over = ball_idx % 6\n",
    "        if ball_idx >= seq_length - 1:\n",
    "            if ball_in_over == 5 or ball_idx == ball_indices[-1]:\n",
    "                over_predictions[over] = prediction\n",
    "    return over_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(actual_values, predicted_values):\n",
    "    return np.sqrt(mean_squared_error(actual_values, predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(overs, model_preds, espn_preds, actual_final_score,\n",
    "                         model_rmse, espn_rmse, comparison_rmse, match_id, innings):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(overs, model_preds, 'ro-', label='Model Predictions', linewidth=2, markersize=8)\n",
    "    plt.plot(overs, espn_preds, 'bs-', label='ESPN Predictions', linewidth=2, markersize=8)\n",
    "    plt.axhline(y=actual_final_score, color='green', linestyle='-', linewidth=3,\n",
    "                label=f'Actual Final Score: {actual_final_score:.0f}')\n",
    "    model_errors = np.abs(np.array(model_preds) - actual_final_score)\n",
    "    espn_errors = np.abs(np.array(espn_preds) - actual_final_score)\n",
    "    plt.fill_between(overs, np.array(model_preds) - model_errors, np.array(model_preds) + model_errors,\n",
    "                     color='red', alpha=0.1)\n",
    "    plt.fill_between(overs, np.array(espn_preds) - espn_errors, np.array(espn_preds) + espn_errors,\n",
    "                     color='blue', alpha=0.1)\n",
    "    plt.text(0.02, 0.97, f\"Model RMSE: {model_rmse:.2f}\", transform=plt.gca().transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.text(0.02, 0.92, f\"ESPN RMSE: {espn_rmse:.2f}\", transform=plt.gca().transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.text(0.02, 0.87, f\"Model vs ESPN RMSE: {comparison_rmse:.2f}\", transform=plt.gca().transAxes,\n",
    "             fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    plt.xlabel('Over', fontsize=14)\n",
    "    plt.ylabel('Predicted Final Score', fontsize=14)\n",
    "    plt.title(f'Model vs ESPN Predictions - Match {match_id}, Innings {innings}', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.savefig('comparison_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43643e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_espn(ball_indices_sorted, pred_sorted, actual_final_score, selected_match, selected_inning, seq_length=30):\n",
    "    model_over_predictions = calculate_over_wise_predictions(ball_indices_sorted, pred_sorted, seq_length)\n",
    "    try:\n",
    "        espn_file_path = r\"/content/Score.xlsx\"\n",
    "        espn_data = load_espn_data(espn_file_path, selected_match, selected_inning)\n",
    "        if espn_data.empty:\n",
    "            print(f\"No ESPN data found for match {selected_match}, inning {selected_inning}\")\n",
    "            return None, None, None\n",
    "        starting_over = (seq_length // 6)\n",
    "        matched_overs = []\n",
    "        model_preds = []\n",
    "        espn_preds = []\n",
    "        for over in espn_data['over']:\n",
    "            if over >= starting_over and over in model_over_predictions:\n",
    "                matched_overs.append(over)\n",
    "                model_preds.append(model_over_predictions[over])\n",
    "                espn_preds.append(espn_data.loc[espn_data['over'] == over, 'espn_prediction'].values[0])\n",
    "        if not matched_overs:\n",
    "            print(f\"No overlapping overs found for comparison in match {selected_match}, inning {selected_inning}\")\n",
    "            return None, None, None\n",
    "        model_rmse = calculate_rmse([actual_final_score] * len(model_preds), model_preds)\n",
    "        espn_rmse = calculate_rmse([actual_final_score] * len(espn_preds), espn_preds)\n",
    "        comparison_rmse = calculate_rmse(espn_preds, model_preds)\n",
    "        visualize_comparison(matched_overs, model_preds, espn_preds, actual_final_score,\n",
    "                            model_rmse, espn_rmse, comparison_rmse, selected_match, selected_inning)\n",
    "        return model_rmse, espn_rmse, comparison_rmse\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ESPN data file not found at: {espn_file_path}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error comparing with ESPN data: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detailed_rmse_analysis(ball_indices_sorted, pred_sorted, actual_final_score, selected_match,\n",
    "                                 selected_inning, espn_data=None, seq_length=30):\n",
    "    phases = {\n",
    "        'Powerplay (1-6)': (1, 6),\n",
    "        'Middle (7-15)': (7, 15),\n",
    "        'Death (16-20)': (16, 20)\n",
    "    }\n",
    "    over_indices = [(idx // 6) + 1 for idx in ball_indices_sorted]\n",
    "    phase_results = {phase: {'model_rmse': None, 'espn_rmse': None, 'comparison_rmse': None}\n",
    "                    for phase in phases.keys()}\n",
    "    for phase_name, (start_over, end_over) in phases.items():\n",
    "        phase_indices = [i for i, over in enumerate(over_indices)\n",
    "                         if start_over <= over <= end_over and ball_indices_sorted[i] >= seq_length - 1]\n",
    "        if not phase_indices:\n",
    "            continue\n",
    "        phase_pred = [pred_sorted[i] for i in phase_indices]\n",
    "        phase_overs = [over_indices[i] for i in phase_indices]\n",
    "        model_rmse = calculate_rmse([actual_final_score] * len(phase_pred), phase_pred)\n",
    "        phase_results[phase_name]['model_rmse'] = model_rmse\n",
    "        if espn_data is not None:\n",
    "            model_over_pred = calculate_over_wise_predictions(\n",
    "                [ball_indices_sorted[i] for i in phase_indices],\n",
    "                [pred_sorted[i] for i in phase_indices],\n",
    "                seq_length\n",
    "            )\n",
    "            espn_phase = espn_data[(espn_data['over'] >= start_over) & (espn_data['over'] <= end_over)]\n",
    "            matched_overs = []\n",
    "            model_preds = []\n",
    "            espn_preds = []\n",
    "            for over in espn_phase['over']:\n",
    "                if over in model_over_pred:\n",
    "                    matched_overs.append(over)\n",
    "                    model_preds.append(model_over_pred[over])\n",
    "                    espn_preds.append(espn_phase.loc[espn_phase['over'] == over, 'espn_prediction'].values[0])\n",
    "            if matched_overs:\n",
    "                espn_rmse = calculate_rmse([actual_final_score] * len(espn_preds), espn_preds)\n",
    "                comparison_rmse = calculate_rmse(espn_preds, model_preds)\n",
    "                phase_results[phase_name]['espn_rmse'] = espn_rmse\n",
    "                phase_results[phase_name]['comparison_rmse'] = comparison_rmse\n",
    "    print(\"\\n=== Detailed RMSE Analysis by Innings Phase ===\")\n",
    "    for phase, results in phase_results.items():\n",
    "        print(f\"\\n{phase}:\")\n",
    "        print(f\"  Model RMSE: {results['model_rmse']:.2f}\" if results['model_rmse'] else \"  Model RMSE: N/A\")\n",
    "        print(f\"  ESPN RMSE: {results['espn_rmse']:.2f}\" if results['espn_rmse'] else \"  ESPN RMSE: N/A\")\n",
    "        print(f\"  Model vs ESPN RMSE: {results['comparison_rmse']:.2f}\" if results['comparison_rmse'] else \"  Model vs ESPN RMSE: N/A\")\n",
    "    return phase_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Load and preprocess full dataset\n",
    "    csv_path = r\"/content/ball_by_ball.csv\"\n",
    "    df = load_and_add_recency(csv_path)\n",
    "    df, feature_cols, encoders = preprocess_data(df)\n",
    "\n",
    "    # 2. Ask user for the starting match_id\n",
    "    initial_match_id = int(input(\"Enter the starting match_id: \"))\n",
    "\n",
    "    # 3. Build sorted list of all match_ids â‰¥ the one entered\n",
    "    all_ids = sorted(df['match_id'].unique())\n",
    "    if initial_match_id not in all_ids:\n",
    "        raise ValueError(f\"match_id {initial_match_id} not found in data\")\n",
    "    start_idx = all_ids.index(initial_match_id)\n",
    "    to_process = all_ids[start_idx:]\n",
    "\n",
    "    # 4. Loop over each match_id, training on all earlier matches and predicting on the current one\n",
    "    seq_length = 30\n",
    "    for match_id in to_process:\n",
    "        print(f\"\\n=== Processing match_id {match_id} ===\")\n",
    "\n",
    "        # 4a. Prepare test sequences for this match\n",
    "        last_match_df = df[df['match_id'] == match_id]\n",
    "        X_last, y_last, w_last, rr_last, wr_last, meta_last = create_sequences_with_weight(\n",
    "            last_match_df, feature_cols=feature_cols, seq_length=seq_length, runs_col='total_run'\n",
    "        )\n",
    "        if len(meta_last) == 0:\n",
    "            print(f\"No sequences for match {match_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 4b. Prepare train+val on all matches with id < current\n",
    "        df_past = df[df['match_id'] < match_id]\n",
    "        X_pv, y_pv, w_pv, rr_pv, wr_pv, meta_pv = create_sequences_with_weight(\n",
    "            df_past, feature_cols=feature_cols, seq_length=seq_length, runs_col='total_run'\n",
    "        )\n",
    "        if len(X_pv) < 1:\n",
    "            print(f\"Not enough past data before match {match_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 4c. Split into train & val\n",
    "        X_train, X_val, y_train, y_val, w_train, w_val, rr_train, rr_val, wr_train, wr_val, _, _ = \\\n",
    "            train_test_split(\n",
    "                X_pv, y_pv, w_pv, rr_pv, wr_pv, meta_pv,\n",
    "                test_size=0.2, random_state=42\n",
    "            )\n",
    "\n",
    "        # 4d. Create DataLoaders\n",
    "        train_ds = CricketBallDatasetWithWeight(X_train, y_train, w_train, rr_train, wr_train)\n",
    "        val_ds   = CricketBallDatasetWithWeight(X_val,   y_val,   w_val,   rr_val,   wr_val)\n",
    "        test_ds  = CricketBallDatasetWithWeight(X_last,  y_last,  w_last,  rr_last,  wr_last)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n",
    "\n",
    "        # 4e. Instantiate and train model\n",
    "        model = TransformerHybridModel(input_size=len(feature_cols))\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model, train_losses, val_losses = train_advanced_model(\n",
    "            model, train_loader, val_loader, epochs=50, device=device\n",
    "        )\n",
    "\n",
    "        # 4f. Plot & save training/validation loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses,   label='Validation Loss')\n",
    "        plt.title(f'Model Loss for match {match_id}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'loss_plot_{match_id}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # 4g. Predict on the current match\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_last, dtype=torch.float32).to(device)\n",
    "            preds = model(X_tensor).cpu().numpy()\n",
    "\n",
    "        # 4h. Sort predictions by ball index\n",
    "        ball_idxs = [m[2] + seq_length - 1 for m in meta_last]\n",
    "        order = sorted(range(len(ball_idxs)), key=lambda i: ball_idxs[i])\n",
    "        ball_idxs_sorted = [ball_idxs[i] for i in order]\n",
    "        preds_sorted = preds[order]\n",
    "\n",
    "        # 4i. Print per-ball predictions\n",
    "        print(f\"\\nPredictions for match {match_id}:\")\n",
    "        for b, p in zip(ball_idxs_sorted, preds_sorted):\n",
    "            print(f\"  Ball {b+1}: {p:.2f}\")\n",
    "\n",
    "        # 4j. Plot predictions vs actual + wickets\n",
    "        group = df[(df['match_id'] == match_id) & (df['original_innings'] == meta_last[0][1])].sort_values('total_balls')\n",
    "        actual_score = group['total_run'].sum()\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.axhline(y=actual_score, linestyle='-', label='Actual Final Score')\n",
    "        plt.plot(ball_idxs_sorted, preds_sorted, marker='x', linestyle='--', label='Predicted Final Score')\n",
    "\n",
    "        wk_indices = group.reset_index(drop=True).index[group['is_wicket'] == 1].tolist()\n",
    "        wk_indices = [i for i in wk_indices if i >= seq_length - 1]\n",
    "        wk_x, wk_p = [], []\n",
    "        for idx in wk_indices:\n",
    "            if idx in ball_idxs_sorted:\n",
    "                pos = ball_idxs_sorted.index(idx)\n",
    "                wk_x.append(idx)\n",
    "                wk_p.append(preds_sorted[pos])\n",
    "        if wk_x:\n",
    "            plt.scatter(wk_x, wk_p, marker='o', s=80, label='Wicket')\n",
    "\n",
    "        plt.xlabel('Ball Index')\n",
    "        plt.ylabel('Final Score')\n",
    "        plt.title(f'Pred vs Actual for match {match_id}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'prediction_plot_{match_id}.png')\n",
    "        plt.close()\n",
    "\n",
    "        # 4k. Summary RMSE\n",
    "        rmse = calculate_rmse([actual_score]*len(preds_sorted), preds_sorted)\n",
    "        print(f\"\\nActual Final Score: {actual_score}\")\n",
    "        print(f\"Predicted Final Score (last ball): {preds_sorted[-1]:.2f}\")\n",
    "        print(f\"Model RMSE: {rmse:.2f}\")\n",
    "\n",
    "        # 4l. (Optional) Compare with ESPN if data exists\n",
    "        try:\n",
    "            espn_df = load_espn_data(r\"/content/Score.xlsx\", match_id, meta_last[0][1])\n",
    "            if not espn_df.empty:\n",
    "                print(\"\\nComparing vs ESPN predictions:\")\n",
    "                m_rmse, e_rmse, c_rmse = compare_with_espn(\n",
    "                    ball_idxs_sorted, preds_sorted, actual_score, match_id, meta_last[0][1], seq_length\n",
    "                )\n",
    "                print(f\"  Model RMSE: {m_rmse:.2f}, ESPN RMSE: {e_rmse:.2f}, Comparison RMSE: {c_rmse:.2f}\")\n",
    "                create_detailed_rmse_analysis(\n",
    "                    ball_idxs_sorted, preds_sorted, actual_score,\n",
    "                    match_id, meta_last[0][1], espn_df, seq_length\n",
    "                )\n",
    "            else:\n",
    "                print(f\"No ESPN data for match {match_id}\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"ESPN data file not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ESPN comparison error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d9839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
